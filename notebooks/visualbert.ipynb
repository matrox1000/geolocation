{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tez","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T12:40:59.375857Z","iopub.execute_input":"2022-05-02T12:40:59.376179Z","iopub.status.idle":"2022-05-02T12:41:09.082368Z","shell.execute_reply.started":"2022-05-02T12:40:59.376146Z","shell.execute_reply":"2022-05-02T12:41:09.081061Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tez in /opt/conda/lib/python3.7/site-packages (0.6.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Everything becomes easy and intuitive from here. \n# Also, Tez keeps your code clean and readable!\n# Let's import a few things.\n\nimport glob\nimport os\n\nimport albumentations\nfrom transformers import BertTokenizer, VisualBertForPreTraining, VisualBertModel\nimport torch\nimport torch.nn as nn\nfrom sklearn import metrics, preprocessing, model_selection\n\nfrom tez import Tez, TezConfig\nfrom tez.callbacks import EarlyStopping\nfrom tez.datasets import ImageDataset\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:41:13.018563Z","iopub.execute_input":"2022-05-02T12:41:13.018918Z","iopub.status.idle":"2022-05-02T12:41:14.160448Z","shell.execute_reply.started":"2022-05-02T12:41:13.018885Z","shell.execute_reply":"2022-05-02T12:41:14.159450Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = \"../input/instacities1m/\"\nIMAGE_PATH = \"../input/instacities1m/InstaCities1M/img_resized_1M/cities_instagram\"\nMODEL_PATH = \"../working/\"\nMODEL_NAME = \"vit_base_patch16_224\"\n#MODEL_NAME = os.path.basename(__file__)[:-3]\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nEPOCHS = 20\nIMAGE_SIZE = 300\nIMAGE_SIZE_MODEL=224","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:41:15.388030Z","iopub.execute_input":"2022-05-02T12:41:15.388380Z","iopub.status.idle":"2022-05-02T12:41:15.394886Z","shell.execute_reply.started":"2022-05-02T12:41:15.388346Z","shell.execute_reply":"2022-05-02T12:41:15.393916Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Let's define a model now\n# We inherit from tez.Model instead of nn.Module\n# we have monitor_metrics if we want to monitor any metrics\n# except the loss\n# and we return 3 values in forward function.\n\nclass InstaModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)\n        \n    def monitor_metrics(self, outputs, targets):\n        device = targets.get_device()\n        if targets is None:\n            return {}\n        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy()\n        accuracy = metrics.accuracy_score(targets, outputs)\n        return {\"accuracy\": torch.tensor(accuracy, device=device)}\n    \n    def optimizer_scheduler(self):\n        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            opt,\n            factor=0.5,\n            patience=2,\n            verbose=True,\n            mode=\"max\",\n            threshold=1e-4,\n        )\n        return opt, sch\n  \n    def forward(self, image, targets=None):\n\n        o_2 = self.model(image)\n        b_o = self.bert_drop(o_2)\n        outputs = self.out(b_o)\n        \n        if targets is not None:\n            loss = nn.CrossEntropyLoss()(outputs, targets)\n            metrics = self.monitor_metrics(outputs, targets)\n            return outputs, loss, metrics\n        return outputs, None, None","metadata":{"execution":{"iopub.status.busy":"2022-05-02T07:37:59.214713Z","iopub.execute_input":"2022-05-02T07:37:59.215351Z","iopub.status.idle":"2022-05-02T07:37:59.230631Z","shell.execute_reply.started":"2022-05-02T07:37:59.215314Z","shell.execute_reply":"2022-05-02T07:37:59.229650Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:07:11.708125Z","iopub.execute_input":"2022-05-02T08:07:11.708440Z","iopub.status.idle":"2022-05-02T08:07:14.038571Z","shell.execute_reply.started":"2022-05-02T08:07:11.708409Z","shell.execute_reply":"2022-05-02T08:07:14.037456Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2022-05-02T08:07:20.682932Z","iopub.execute_input":"2022-05-02T08:07:20.683221Z","iopub.status.idle":"2022-05-02T08:07:20.694212Z","shell.execute_reply.started":"2022-05-02T08:07:20.683191Z","shell.execute_reply":"2022-05-02T08:07:20.692897Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"VisualBertModel(\n  (embeddings): VisualBertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=1)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (visual_token_type_embeddings): Embedding(2, 768)\n    (visual_position_embeddings): Embedding(512, 768)\n    (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n  )\n  (encoder): VisualBertEncoder(\n    (layer): ModuleList(\n      (0): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): VisualBertLayer(\n        (attention): VisualBertAttention(\n          (self): VisualBertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): VisualBertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): VisualBertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): VisualBertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): VisualBertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"dfx = pd.read_csv(INPUT_PATH + \"train.csv\")\ndfx = dfx.dropna().reset_index(drop=True)\ndfx[\"path\"] = dfx[\"category\"].astype(str) + \"/\" + dfx[\"id\"].astype(str) + \".jpg\"\n    \nlbl_enc = preprocessing.LabelEncoder()\ndfx.category = lbl_enc.fit_transform(dfx.category.values)\n\n\n\ndf_train, df_valid = model_selection.train_test_split(\n    dfx, test_size=0.1, random_state=42, stratify=dfx.category.values\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\n\n\ntrain_image_paths = [os.path.join(IMAGE_PATH, x ) for x in df_train.path.values]\nvalid_image_paths = [os.path.join(IMAGE_PATH, x ) for x in df_valid.path.values]\ntrain_targets = df_train.category.values\nvalid_targets = df_valid.category.values\n\ndataset_aug = albumentations.Compose(\n    [\n    albumentations.Resize(IMAGE_SIZE_MODEL, IMAGE_SIZE_MODEL)\n    ]\n)\n\n\ntrain_dataset = ImageDataset(\n    image_paths=train_image_paths,\n    targets=train_targets,\n    augmentations=dataset_aug,\n    backend=\"cv2\"\n\n\n)\n\nvalid_dataset = ImageDataset(\n    image_paths=valid_image_paths,\n    targets=valid_targets,\n    augmentations=dataset_aug,\n    backend=\"cv2\"\n)\n\nmodel = InstaModel(num_classes=dfx.category.nunique())\nes = EarlyStopping(\n    monitor=\"valid_loss\",\n    model_path=os.path.join(MODEL_PATH, MODEL_NAME + \".bin\"),\n    patience=3,\n    mode=\"min\",\n)\n\nmodel = Tez(model)\nconfig = TezConfig(\n    training_batch_size=TRAIN_BATCH_SIZE,\n    validation_batch_size=VALID_BATCH_SIZE,\n    epochs=EPOCHS,\n    step_scheduler_after=\"epoch\",\n    step_scheduler_metric=\"valid_loss\",\n)\n\n\nmodel.fit(\n    train_dataset,\n    valid_dataset=valid_dataset,\n    device=\"cuda\",\n    config=config,\n    callbacks=[es],\n)\nmodel.save(os.path.join(MODEL_PATH, MODEL_NAME + \"_image.bin\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, VisualBertModel\n\nmodel = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ninputs = tokenizer(\"What is the man eating?\", return_tensors=\"pt\")\n# this is a custom function that returns the visual embeddings given the image path\nimage_path=\"../input/instacities1m/InstaCities1M/img_resized_1M/cities_instagram/chicago/1481574059510467614.jpg\"\nvisual_embeds = get_visual_embeddings(image_path)\n\nvisual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\nvisual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\ninputs.update(\n     {\n         \"visual_embeds\": visual_embeds,\n         \"visual_token_type_ids\": visual_token_type_ids,\n         \"visual_attention_mask\": visual_attention_mask,\n     }\n)\noutputs = model(**inputs)\nlast_hidden_state = outputs.last_hidden_state","metadata":{"execution":{"iopub.status.busy":"2022-05-02T10:07:20.700791Z","iopub.execute_input":"2022-05-02T10:07:20.701136Z","iopub.status.idle":"2022-05-02T10:08:10.767598Z","shell.execute_reply.started":"2022-05-02T10:07:20.701049Z","shell.execute_reply":"2022-05-02T10:08:10.766264Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/631 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec4e186cf704745ad5f8293792a1f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/428M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e383616aa0402cbe99d00fe51351ab"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369d3ec67be6408bb2563d0a28163da2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58e87adf8bb14efc920e6aa22f3a162c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ccc00f5dc64e3dac668c16c9f817f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f9efee889a43379be2c8cec5fd5e51"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/2984043440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# this is a custom function that returns the visual embeddings given the image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../input/instacities1m/InstaCities1M/img_resized_1M/cities_instagram/chicago/1481574059510467614.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvisual_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_visual_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvisual_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_visual_embeddings' is not defined"],"ename":"NameError","evalue":"name 'get_visual_embeddings' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch\nfrom PIL import Image, ImageFile\n\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass InstaDataset:\n    def __init__(\n        self,\n        image_paths,\n        text,\n        targets,\n        tokenizer,\n        max_len,\n        augmentations=None,\n        backend=\"pil\",\n        channel_first=True,\n        grayscale=False,\n    ):\n        \"\"\"\n        :param image_paths: list of paths to images\n        :param targets: numpy array\n        :param augmentations: albumentations augmentations\n        \"\"\"\n        self.image_paths = image_paths\n        self.targets = targets\n        self.augmentations = augmentations\n        self.backend = backend\n        self.channel_first = channel_first\n        self.grayscale = grayscale\n        self.text = text\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, item):\n        targets = self.targets[item]\n        if self.backend == \"pil\":\n            image = Image.open(self.image_paths[item])\n            image = np.array(image)\n            if self.augmentations is not None:\n                augmented = self.augmentations(image=image)\n                image = augmented[\"image\"]\n        elif self.backend == \"cv2\":\n            if self.grayscale is False:\n                image = cv2.imread(self.image_paths[item])\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            else:\n                image = cv2.imread(self.image_paths[item], cv2.IMREAD_GRAYSCALE)\n            if self.augmentations is not None:\n                augmented = self.augmentations(image=image)\n                image = augmented[\"image\"]\n        else:\n            raise Exception(\"Backend not implemented\")\n        if self.channel_first is True and self.grayscale is False:\n            image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n\n        image_tensor = torch.tensor(image)\n        if self.grayscale:\n            image_tensor = image_tensor.unsqueeze(0)\n            \n        #text\n        text = str(self.text[item])\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        \n        \n        return {\n            \"image\": image_tensor,\n            \"targets\": torch.tensor(targets),\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:41:35.208210Z","iopub.execute_input":"2022-05-02T12:41:35.208548Z","iopub.status.idle":"2022-05-02T12:41:35.229522Z","shell.execute_reply.started":"2022-05-02T12:41:35.208516Z","shell.execute_reply":"2022-05-02T12:41:35.228474Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn import metrics, model_selection\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n\nfrom tez import Tez, TezConfig\nfrom tez.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:41:41.177233Z","iopub.execute_input":"2022-05-02T12:41:41.177718Z","iopub.status.idle":"2022-05-02T12:41:41.185660Z","shell.execute_reply.started":"2022-05-02T12:41:41.177656Z","shell.execute_reply":"2022-05-02T12:41:41.184461Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class args:\n    tokenizer = \"bert-base-uncased\"\n    model = \"uclanlp/visualbert-vqa-coco-pre\"\n    epochs = 20\n    batch_size = 32\n    learning_rate = 5e-5\n    train_batch_size = 32\n    valid_batch_size = 32\n    max_len = 128\n    accumulation_steps = 1\n\n\nclass InstaModel(nn.Module):\n    def __init__(self, num_classes, model_name, num_train_steps, learning_rate):\n        super().__init__()\n        self.num_train_steps = num_train_steps\n        self.learning_rate = learning_rate\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n\n        config = AutoConfig.from_pretrained(model_name)\n\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n                \"num_labels\": num_classes\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.output = nn.Linear(config.hidden_size, num_classes)\n\n    def optimizer_scheduler(self):\n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        opt = torch.optim.AdamW(optimizer_parameters, lr=self.learning_rate)\n        sch = get_linear_schedule_with_warmup(\n            opt,\n            num_warmup_steps=0,\n            num_training_steps=self.num_train_steps,\n        )\n\n        return opt, sch\n\n    def loss(self, outputs, targets):\n        if targets is None:\n            return None\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def monitor_metrics(self, outputs, targets):\n        if targets is None:\n            return {}\n        device = targets.get_device()\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy() >= 0.5\n        targets = targets.cpu().detach().numpy()\n        accuracy = metrics.accuracy_score(targets, outputs)\n        return {\"accuracy\": torch.tensor(accuracy, device=device)}\n\n    def forward(self, ids, mask, token_type_ids, targets=None):\n        transformer_out = self.transformer(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids,\n        )\n        out = transformer_out.pooler_output\n        out = self.dropout(out)\n        output = self.output(out)\n        loss = self.loss(output, targets)\n        acc = self.monitor_metrics(output, targets)\n        return output, loss, acc\n\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:41:44.698811Z","iopub.execute_input":"2022-05-02T12:41:44.699131Z","iopub.status.idle":"2022-05-02T12:41:44.719692Z","shell.execute_reply.started":"2022-05-02T12:41:44.699097Z","shell.execute_reply":"2022-05-02T12:41:44.718557Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dfx = pd.read_csv(INPUT_PATH + \"train.csv\")\ndfx = dfx.dropna().reset_index(drop=True)\ndfx[\"path\"] = dfx[\"category\"].astype(str) + \"/\" + dfx[\"id\"].astype(str) + \".jpg\"\n    \nlbl_enc = preprocessing.LabelEncoder()\ndfx.category = lbl_enc.fit_transform(dfx.category.values)\n\ndf_train, df_valid = model_selection.train_test_split(\n    dfx, test_size=0.1, random_state=42, stratify=dfx.category.values\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntrain_image_paths = [os.path.join(IMAGE_PATH, x ) for x in df_train.path.values]\nvalid_image_paths = [os.path.join(IMAGE_PATH, x ) for x in df_valid.path.values]\ntrain_targets = df_train.category.values\nvalid_targets = df_valid.category.values\n\ndataset_aug = albumentations.Compose(\n    [\n    albumentations.Resize(IMAGE_SIZE_MODEL, IMAGE_SIZE_MODEL)\n    ]\n)\n\ntokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n\ntrain_dataset = InstaDataset(\n    image_paths=train_image_paths,\n    targets=train_targets,\n    augmentations=dataset_aug,\n    backend=\"cv2\",\n    text=df_train.text.values,\n    tokenizer=tokenizer,\n    max_len=args.max_len,\n\n)\n\nvalid_dataset = InstaDataset(\n    image_paths=valid_image_paths,\n    targets=valid_targets,\n    augmentations=dataset_aug,\n    backend=\"cv2\",\n    text=df_valid.text.values,\n    tokenizer=tokenizer,\n    max_len=args.max_len,\n)\n\nn_train_steps = int(len(train_dataset) / args.batch_size / args.accumulation_steps * args.epochs)\n\nmodel = InstaModel(\n        model_name=args.model,\n        num_train_steps=n_train_steps,\n        learning_rate=args.learning_rate,\n        num_classes=dfx.category.nunique()\n    )\nmodel = Tez(model)\nes = EarlyStopping(monitor=\"valid_loss\", model_path=\"model.bin\")\nconfig = TezConfig(\n        training_batch_size=args.train_batch_size,\n        validation_batch_size=args.valid_batch_size,\n        gradient_accumulation_steps=args.accumulation_steps,\n        epochs=args.epochs,\n        step_scheduler_after=\"batch\",\n    )\n\n    \nmodel.fit(\n    train_dataset,\n    valid_dataset=valid_dataset,\n    device=\"cuda\",\n    config=config,\n    callbacks=[es],\n)\n\nmodel.save(os.path.join(MODEL_PATH, MODEL_NAME + \"_image.bin\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:44:55.309871Z","iopub.execute_input":"2022-05-02T12:44:55.310994Z","iopub.status.idle":"2022-05-02T12:45:18.884308Z","shell.execute_reply.started":"2022-05-02T12:44:55.310937Z","shell.execute_reply":"2022-05-02T12:45:18.882775Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n2022-05-02 12:45:12,019 INFO Using single GPU\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/506250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d542fb66e4b84d4a94233b20f63bb7a2"}},"metadata":{}},{"name":"stderr","text":"2022-05-02 12:45:17,448 INFO \nTezConfig(device='cuda', training_batch_size=32, validation_batch_size=32, test_batch_size=32, epochs=20, gradient_accumulation_steps=1, clip_grad_norm=-1, num_jobs=2, fp16=False, train_shuffle=True, valid_shuffle=True, train_drop_last=False, valid_drop_last=False, test_drop_last=False, test_shuffle=False, pin_memory=True, step_scheduler_after='batch', step_scheduler_metric=None, val_strategy='epoch', val_steps=100)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1265850229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tez/model/tez.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataset, valid_dataset, config, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_START\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loader\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tez/model/tez.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_STEP_START\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_loss_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN_STEP_END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tez/model/tez.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tez/model/tez.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'image'"],"ename":"TypeError","evalue":"forward() got an unexpected keyword argument 'image'","output_type":"error"}]},{"cell_type":"code","source":"AutoTokenizer.from_pretrained(args.model)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:11:33.617417Z","iopub.execute_input":"2022-05-02T12:11:33.617746Z","iopub.status.idle":"2022-05-02T12:11:34.563769Z","shell.execute_reply.started":"2022-05-02T12:11:33.617704Z","shell.execute_reply":"2022-05-02T12:11:34.562486Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1787988139.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_class_to_model_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reverse_config_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: <class 'transformers.models.visual_bert.configuration_visual_bert.VisualBertConfig'>"],"ename":"KeyError","evalue":"<class 'transformers.models.visual_bert.configuration_visual_bert.VisualBertConfig'>","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}