{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tez\nimport pandas as pd\nimport numpy as np\nimport tez\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom sklearn import metrics, model_selection, preprocessing\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n\nclass BERTDataset:\n    def __init__(self, text, target):\n        self.text = text\n        self.target = target\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.max_len = 64\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(self.target[item], dtype=torch.long),\n        }\n\n\nclass BERTBaseUncased(tez.Model):\n    def __init__(self, num_train_steps, num_classes):\n        super().__init__()\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", do_lower_case=True\n        )\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)\n\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n    def fetch_optimizer(self):\n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        opt = AdamW(optimizer_parameters, lr=3e-5)\n        return opt\n\n    def fetch_scheduler(self):\n        sch = get_linear_schedule_with_warmup(\n            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\n        )\n        return sch\n\n    def loss(self, outputs, targets):\n        if targets is None:\n            return None\n        return nn.CrossEntropyLoss()(outputs, targets)\n\n    def monitor_metrics(self, outputs, targets):\n        if targets is None:\n            return {}\n        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy()\n        accuracy = metrics.accuracy_score(targets, outputs)\n        return {\"accuracy\": accuracy}\n\n    def forward(self, ids, mask, token_type_ids, targets=None):\n        _, o_2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        b_o = self.bert_drop(o_2)\n        output = self.out(b_o)\n        loss = self.loss(output, targets)\n        acc = self.monitor_metrics(output, targets)\n        return output, loss, acc\n    \n    def process_output(self, output):\n        output = torch.argmax(output, dim=1).cpu().detach().numpy()\n        return output\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-30T18:21:36.558769Z","iopub.execute_input":"2022-03-30T18:21:36.559095Z","iopub.status.idle":"2022-03-30T18:21:44.165592Z","shell.execute_reply.started":"2022-03-30T18:21:36.559063Z","shell.execute_reply":"2022-03-30T18:21:44.164735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nINPUT_PATH = \"../input/instacities1m/\"\nIMAGE_PATH = \"../input/instacities1m/InstaCities1M/img_resized_1M/cities_instagram\"\nMODEL_PATH = \"../working/\"\nMODEL_NAME = \"resnet18\"\n#MODEL_NAME = os.path.basename(__file__)[:-3]\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nEPOCHS = 20\nIMAGE_SIZE = 300","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:21:44.170905Z","iopub.execute_input":"2022-03-30T18:21:44.172923Z","iopub.status.idle":"2022-03-30T18:21:44.179479Z","shell.execute_reply.started":"2022-03-30T18:21:44.172884Z","shell.execute_reply":"2022-03-30T18:21:44.178847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndfx = pd.read_csv(INPUT_PATH + \"test.csv\", nrows=100)\ndfx = dfx.dropna().reset_index(drop=True)\ndfx[\"path\"] = dfx[\"category\"].astype(str) + \"/\" + dfx[\"id\"].astype(str) + \".jpg\"\n    \nlbl_enc = preprocessing.LabelEncoder()\ndfx.category = lbl_enc.fit_transform(dfx.category.values)\n\n\n\n\n\ntest_image_paths = [os.path.join(IMAGE_PATH, x ) for x in dfx.path.values]\ntest_targets = dfx.category.values\n\n\n\n\ntest_dataset = BERTDataset(\n    text=dfx.text.values, target=dfx.category.values\n)\n\nmodel = BERTBaseUncased(\n    num_train_steps=0, num_classes=dfx.category.nunique()\n)\n\n#model.load(\"/root/dataset/model_only_text.bin\", device=\"cpu\")\nmodel.load(\"../input/modelonlytext/model_only_text_categorical.bin\")\n\npreds = model.predict(test_dataset, batch_size=32, n_jobs=-1)\n#print(model.monitor_metrics(preds, dfx.category.values))\npredictions = list()\nfor yhat in preds:\n    predictions.extend(yhat)\n    #yhatt = np.argmax(preds, axis=1)\n    #yhat=list(preds)\n    #print(type(predictions))\n    #print(len(predictions))\n    #\n    #print (predictions)\n    #print(type(dfx.category.values))\n    #print(len(dfx.category.values))\n    #print(dfx.category.values)\n    \n    #    else:\n#        final_preds = np.vstack((final_preds, p))\n#final_preds = final_preds.argmax(axis=1)\nnp.savetxt(\"./predictions_text.csv\", \n           predictions,\n           delimiter =\", \", \n           fmt ='% s')\n#print(\"Precision \"+ model_file+ \": \" + str(metrics.accuracy_score(dfx.category.values, predictions)))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:22:44.435369Z","iopub.execute_input":"2022-03-30T18:22:44.435658Z","iopub.status.idle":"2022-03-30T18:22:52.485894Z","shell.execute_reply.started":"2022-03-30T18:22:44.435627Z","shell.execute_reply":"2022-03-30T18:22:52.484845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-03-30T18:22:21.368162Z","iopub.execute_input":"2022-03-30T18:22:21.368490Z","iopub.status.idle":"2022-03-30T18:22:21.403829Z","shell.execute_reply.started":"2022-03-30T18:22:21.368456Z","shell.execute_reply":"2022-03-30T18:22:21.402928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}